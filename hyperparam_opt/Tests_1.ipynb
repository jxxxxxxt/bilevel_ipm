{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import load_diabetes, train_val_test_split\n",
    "\n",
    "data_utils = load_diabetes()\n",
    "\n",
    "data_utils.shape\n",
    "\n",
    "data_list=[]\n",
    "\n",
    "f = open(\"diabete.txt\",encoding = \"utf-8\")\n",
    "a_list=f.readlines()\n",
    "f.close()\n",
    "for line in a_list:\n",
    "    line1=line.replace('\\n', '')\n",
    "    line2=list(line1.split(' '))\n",
    "    y=float(line2[0])\n",
    "    x= [float(line2[i].split(':')[1]) for i in (1,2,3,4,5,6,7,8)]\n",
    "    data_list.append(x+[y])\n",
    "\n",
    "\n",
    "data_array_1=np.array(data_list)[:,:-1]\n",
    "data_array_0=np.ones((data_array_1.shape[0],1))\n",
    "data_array_2=data_array_1*data_array_1\n",
    "data_array_3=np.empty((data_array_1.shape[0],0))\n",
    "\n",
    "for i in range(data_array_1.shape[1]):\n",
    "    for j in range(data_array_1.shape[1]):\n",
    "        if i<j:\n",
    "            data_array_i=data_array_1[:,i]*data_array_1[:,j]\n",
    "            data_array_i=np.reshape(data_array_i,(-1,1))\n",
    "            data_array_3=np.hstack((data_array_3,data_array_i))\n",
    "\n",
    "data_array_4=np.reshape(np.array(data_list)[:,-1],(-1,1))\n",
    "data=np.hstack((data_array_0,data_array_1,data_array_2,data_array_3,data_array_4))\n",
    "\n",
    "n_train = 500\n",
    "n_val = 150\n",
    "\n",
    "metrics = []\n",
    "variables = []\n",
    "\n",
    "hparams = {\n",
    "    'gam': 5,\n",
    "    'eta': 0.1,\n",
    "    'alpha': 0.01,\n",
    "    'beta': 0.01,\n",
    "    'epsilon': 1e-5,\n",
    "    'max_iters_outer': 100,\n",
    "    'max_iters_inner': 100,\n",
    "    'M': 0.05,\n",
    "    't': 1e-3\n",
    "}\n",
    "\n",
    "epochs = 80\n",
    "plot_results = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/8m/fg6btdcj0ps98p4v0vdfnpnw0000gn/T/ipykernel_88997/1291107838.py\", line 2, in <module>\n",
      "    from algorithms import barrier_blo\n",
      "  File \"/Users/jiaxiangli/Downloads/bilevel_ipm/bilevel_ipm/hyperparam_opt/algorithms/barrier_blo.py\", line 7, in <module>\n",
      "    import torch\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/cvxpy/reductions/solvers/solving_chain.py:356: FutureWarning: \n",
      "    You specified your problem should be solved by ECOS. Starting in\n",
      "    CXVPY 1.6.0, ECOS will no longer be installed by default with CVXPY.\n",
      "    Please either add ECOS as an explicit install dependency to your project\n",
      "    or switch to our new default solver, Clarabel, by either not specifying a\n",
      "    solver argument or specifying ``solver=cp.CLARABEL``. To suppress this\n",
      "    warning while continuing to use ECOS, you can filter this warning using\n",
      "    Python's ``warnings`` module until you are using 1.6.0.\n",
      "    \n",
      "  warnings.warn(ECOS_DEP_DEPRECATION_MSG, FutureWarning)\n",
      "/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/cvxpy/problems/problem.py:1407: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  warnings.warn(\n",
      "/Users/jiaxiangli/anaconda3/envs/blo/lib/python3.10/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: [-1.93542592e-01  7.57841745e-01 -1.66091484e+00 -2.00691748e+00\n",
      "  3.91593584e-01  1.87836329e-01  2.78740208e-01 -1.92697568e+00\n",
      " -1.07433888e+00  6.40271579e-01  2.51883649e-01 -1.70257478e-01\n",
      " -1.51619608e+00  7.06846955e-01  4.36063554e-01  1.48864265e+00\n",
      "  3.13145560e-01  4.06852992e-01 -6.33631517e-01 -3.46690822e+00\n",
      " -1.60011959e-01  8.55953410e-03 -8.22694097e-02  1.03946869e+00\n",
      " -7.45756986e-01  1.12740581e+00  1.65472824e-01 -1.82652647e-01\n",
      " -1.69753474e+00 -1.41940034e+00  2.59361675e-01  6.57771032e-01\n",
      "  1.38284910e+00 -5.86837286e-01 -1.38381186e-01  3.44201628e-01\n",
      "  1.66877732e+00 -2.86810405e-01  6.68724820e-02  5.67423953e-01\n",
      "  9.31812777e-01  6.12995356e-01  1.04923360e+00 -9.31312145e-01\n",
      " -9.47580242e-01 -2.46856960e-01  3.06603740e-01  3.32551950e-01\n",
      " -1.35881356e+00 -1.55767613e+00  1.08519994e+00 -2.54999635e-01\n",
      "  8.89748681e-01  2.64949535e-01 -5.39630896e-02  7.69069431e-01\n",
      " -1.08090994e+00 -2.13443500e-01  2.65785625e+00  9.79114118e-01\n",
      "  1.35455954e+00  4.71387288e-01 -6.52540104e-02  1.03209545e+00\n",
      "  2.59720761e+00  1.89803987e+00  7.50820239e-01  1.19024495e-01\n",
      "  6.11411600e-01  2.62195658e-01 -1.56963462e+00 -6.54047378e-01\n",
      " -3.78879167e-01  5.50429850e-01  1.08217484e+00 -6.52604686e-01\n",
      " -1.04492176e+00  2.14677067e+00  8.59949059e-01  1.32947937e+00\n",
      "  1.32216143e-02  3.73031780e-01 -7.37482397e-02  5.37647442e-02\n",
      "  3.01234887e-01 -3.05625557e-01 -1.28670580e+00 -4.26095712e-01\n",
      " -1.96339914e-01 -4.20376344e-01 -7.26639610e-01  1.52372094e+00\n",
      "  9.30976313e-01  4.17509307e-01 -2.74744192e-01  1.66979275e+00\n",
      "  6.46045362e-01  5.09945462e-01 -1.03468516e+00 -6.43728683e-01\n",
      " -1.73684839e+00  5.10053211e-02  8.77770690e-01  5.85953950e-01\n",
      " -5.12570258e-01  1.35978578e+00  1.10164617e-02 -1.19228884e+00\n",
      "  1.30078172e+00  5.98294471e-01 -2.11510626e+00  8.30438661e-01\n",
      " -5.40348173e-01 -1.40270712e+00 -2.07561719e+00 -2.48231043e-01\n",
      " -6.37025028e-01 -1.35686760e-01  1.32647988e+00  1.05912672e+00\n",
      " -1.46099043e-01 -2.94488664e-01 -5.71576003e-01  2.88370389e-01\n",
      " -5.48482937e-01 -3.23691206e-01  3.75357977e-01 -4.44546905e-01\n",
      "  1.15109902e+00  1.12357800e+00  7.85026325e-01  2.05659934e-01\n",
      " -1.11268858e+00  8.67380012e-01 -1.31333681e+00  6.98101700e-01\n",
      "  1.18710791e-01  1.35897854e-01 -4.38973637e-01  1.82912540e-01\n",
      "  7.77443379e-01  3.66817484e-01 -4.54632655e-01 -4.31498089e-01\n",
      "  7.73515513e-01  7.54864161e-01  1.05364135e-01 -1.01598305e+00\n",
      " -1.15201399e+00 -1.44231003e+00 -1.77573675e-01  3.49863677e-01\n",
      " -1.81984652e+00  1.44290922e+00 -1.17276322e+00 -2.79078027e-01\n",
      "  1.11049080e+00  1.55409326e+00 -8.38413878e-01  4.02060020e-01\n",
      " -6.80662365e-01 -1.69052279e+00 -1.61941639e+00  1.95903315e+00\n",
      "  1.76313620e+00  9.30580765e-01  8.40567109e-01 -9.71339315e-01\n",
      "  4.68089005e-01  1.04546498e-01  6.59103963e-01  1.16341575e+00\n",
      " -2.37892748e+00  8.89580577e-01 -1.97604132e+00 -1.85028636e+00\n",
      " -1.88966679e-01 -8.68008491e-01  5.00625245e-01  8.12399459e-01\n",
      " -3.00101672e-01  1.10063527e+00 -4.05281489e-02  4.91198195e-01\n",
      "  1.70404212e+00 -1.32662402e+00 -2.66245277e-01  1.56720760e+00\n",
      " -2.07732290e-01  1.60223165e+00  1.85624088e-01  1.54089697e+00\n",
      " -2.13210009e+00 -1.47194143e+00 -5.21600743e-01  1.18718882e+00\n",
      " -3.19761923e-01 -8.81798154e-01  1.81966298e+00 -1.75738098e-01\n",
      " -6.68548574e-01 -1.25689402e+00  2.72056497e-01 -1.73360398e-01\n",
      " -1.06908811e+00 -1.23919690e+00 -1.30351973e+00  1.72398297e+00\n",
      " -1.04867571e+00  3.54434566e-01  9.29417680e-01 -1.56137230e+00\n",
      " -1.26185109e+00  7.36429871e-01  1.52615470e-02  1.45752085e+00\n",
      "  7.59803672e-01  3.48531451e-01 -6.54243559e-02 -9.10699878e-01\n",
      "  2.83391367e-01  1.73838999e-02  1.82197334e+00 -5.35823518e-01\n",
      "  1.11458820e-03 -6.17518513e-01 -1.52366124e+00 -2.00555515e+00\n",
      "  7.12749911e-01 -1.42577715e+00  4.09314171e-02 -2.67728772e-01\n",
      " -6.16550652e-02  6.33978534e-01  3.62028359e-02  7.39832034e-01\n",
      " -2.29620091e-01 -2.14108353e+00 -1.49739247e+00  1.37129368e+00\n",
      "  7.69591869e-01  3.81372547e-01 -2.36067248e+00 -1.44864558e-01\n",
      "  8.25389610e-01 -6.03683490e-01  4.03593385e-01 -7.94326184e-01\n",
      " -8.66694619e-01 -7.66687009e-02  1.01991196e+00  6.42920145e-02\n",
      "  8.05404917e-01 -5.61195015e-01  1.88728994e+00 -1.05532457e+00\n",
      "  8.31017798e-01 -4.41578834e-01 -1.61133837e+00  7.40777779e-01\n",
      " -6.49915831e-02  1.01591425e+00 -2.76870420e-01  3.09580648e-01\n",
      "  2.69454629e-03  2.71623160e-01  3.06785674e-01  1.04093447e+00\n",
      "  4.02939906e-01  7.73312468e-01  3.29821374e-01 -2.48351298e-01\n",
      "  4.98267520e-01 -2.09732930e+00 -7.62413023e-01 -1.33204113e+00\n",
      "  2.06039387e-01  9.54808147e-01  3.17791841e-01  3.58230090e-01\n",
      " -7.83794788e-01  6.15313013e-01  4.89716108e-01 -5.48838459e-01\n",
      "  3.41853381e-01 -2.93855044e-01  3.02120763e-01 -5.56739346e-01\n",
      " -1.45650278e+00  1.97259826e+00 -1.21372503e+00  1.96080272e+00\n",
      " -6.05264881e-01 -2.83794033e-01 -3.63575851e-02 -2.53412087e-01\n",
      " -1.01768207e+00 -4.61836569e-01 -1.37165549e-01  8.68472352e-01\n",
      "  1.58010951e+00 -1.06016923e+00  1.31408044e-01 -1.89317973e+00\n",
      "  1.18416301e+00 -3.04029575e-01  1.29400034e-01 -1.63302082e+00\n",
      "  7.62293481e-01 -3.63677104e-01  2.30540901e+00 -1.20701596e+00\n",
      "  9.63072476e-01  9.77003139e-01 -1.43447323e+00  3.93365216e-01\n",
      "  1.60650939e+00  1.70568586e+00  6.50461409e-01 -1.17539694e+00\n",
      " -1.47397129e-01 -5.11162244e-01 -1.37476177e+00  5.33915945e-01\n",
      " -2.69531318e-02 -1.75469482e+00  1.79671191e+00  1.35564559e+00\n",
      "  5.95095951e-01 -5.35080688e-01  7.95469212e-01  8.78337428e-03\n",
      " -4.65006890e-01 -5.16028842e-01 -2.69576034e-01 -1.15936472e+00\n",
      " -7.74804421e-01 -2.42925987e-01  1.33231357e+00 -5.67444245e-01\n",
      " -2.46913309e-01  1.73131157e+00 -1.57165716e+00  6.47219035e-01\n",
      "  1.20361265e+00 -9.27351178e-01 -1.27152323e+00  4.82855906e-01\n",
      "  3.03204253e-01  2.06439186e+00 -9.70179046e-01  2.05554600e+00\n",
      "  1.63395422e+00  1.55042940e+00 -5.89022781e-02  1.20462676e-01\n",
      " -8.17009692e-01 -6.52961415e-01 -2.17789602e-01  3.30286317e-01\n",
      " -1.85638443e-01 -3.38642298e-02  2.56555957e-01 -9.05461398e-01\n",
      " -1.11831054e-01 -1.54552168e-01  1.23193412e-01 -1.40659654e+00\n",
      " -5.20333571e-01  2.15509548e-01 -4.59879205e-01 -2.42907603e-01\n",
      " -7.89833846e-01  1.07493800e+00  1.25116962e+00 -1.64266532e+00\n",
      " -1.15958230e-01 -3.28169866e-01  2.07650018e-01  7.64414937e-02\n",
      " -4.95826632e-01 -3.91346851e-01 -7.61773450e-01  7.73358268e-02\n",
      " -2.02560394e-01  3.14498386e-01 -5.00131189e-01 -8.74492346e-01\n",
      " -6.29133753e-01  1.02146786e+00 -2.40353730e+00 -2.14500303e+00\n",
      "  1.02582485e+00  6.86551395e-01  1.49220937e+00 -3.57389842e-01\n",
      " -2.56331166e-01 -6.10412743e-01 -9.00773436e-01 -3.43075320e-01\n",
      "  5.77393277e-01  2.78103245e-01 -5.98236014e-02  2.17465412e-01\n",
      "  4.76299906e-02  8.07527231e-01 -5.52972963e-02 -9.06743666e-01\n",
      " -8.36056273e-01 -4.40942877e-02  2.16195004e+00 -1.12435122e+00\n",
      "  7.13837017e-01  7.13282983e-01  4.91618866e-01 -1.64372839e-01\n",
      "  4.46113472e-01 -1.05710054e+00  1.23885580e+00  4.15807470e-01\n",
      " -1.39050169e-01  6.62334986e-01 -7.03110217e-01 -2.85916036e-01\n",
      " -1.49217630e+00 -7.43146399e-02 -5.43337263e-01  1.55390203e+00\n",
      "  1.22929160e+00 -7.81995227e-02 -9.00962273e-02  9.46053767e-01\n",
      " -1.30500355e+00  7.68679039e-01  6.17280578e-01  1.35392216e-01\n",
      " -5.42531594e-01 -4.76011369e-01 -9.06733719e-02  2.08120866e+00\n",
      " -1.29688345e-01  2.26711404e+00  2.63883235e-01  2.84894564e-02\n",
      " -2.14849919e-01  1.35010144e-01  8.92936824e-01  1.24501130e+00\n",
      " -7.81106567e-01  6.18326569e-01 -3.96101782e-01 -3.03961676e-02\n",
      " -2.44171136e-01  2.04318763e+00  8.30882320e-01 -2.74372565e-01\n",
      "  5.76715085e-01  7.97443254e-01 -8.76663862e-01 -3.15042536e-02\n",
      "  4.94964680e-02  1.47904677e+00  3.60731021e-01 -4.08364121e-01\n",
      "  1.15447952e+00 -8.38187832e-01 -1.70021577e+00 -2.16612737e-01\n",
      "  1.03333188e+00  7.61552358e-01 -4.39735772e-01 -3.03214881e-01\n",
      "  9.71324205e-01 -1.23016436e-02 -1.05458917e-02  4.41532517e-01\n",
      "  8.90954861e-01  6.46506106e-01 -4.70176288e-01  1.13295544e+00\n",
      " -6.72668492e-01  7.96377984e-01  4.86999605e-01  7.23977231e-02\n",
      " -1.05107536e+00 -8.28458505e-01 -4.06061502e-01  8.82825095e-01\n",
      "  1.39970380e+00 -1.62833130e+00  1.82610759e+00  4.07709001e-01\n",
      " -1.03656989e-01 -2.72194279e-01 -1.98019610e-01 -8.67547423e-01\n",
      " -8.79475886e-02  1.02854715e+00 -2.68451877e+00 -4.89833501e-01], w: [ inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf  inf -inf\n",
      " -inf  inf -inf -inf  inf  inf  inf -inf  inf  inf  inf -inf -inf  inf\n",
      " -inf  inf -inf -inf -inf  inf -inf  inf  inf  inf  inf -inf  inf -inf\n",
      " -inf -inf -inf], b: inf, xi: [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf inf inf inf inf inf inf]\n",
      "constraints: [np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, np.False_, array([False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False, False, False, False, False,\n",
      "       False, False, False, False, False])]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m barrier_blo \u001b[38;5;241m=\u001b[39m barrier_blo\u001b[38;5;241m.\u001b[39mBarrier_BLO(problem, hparams)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# metrics_seed, variables_seed = barrier_blo(x_train, y_train, x_val, y_val, x_test, y_test, hparams, epochs)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m metrics_seed, c, w, b, xi, grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mbarrier_blo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupper_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(metrics_seed)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# variables.append([c, w, b, xi])\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/bilevel_ipm/bilevel_ipm/hyperparam_opt/algorithms/barrier_blo.py:83\u001b[0m, in \u001b[0;36mBarrier_BLO.upper_loop\u001b[0;34m(self, c0, w0, b0, xi0, hparams)\u001b[0m\n\u001b[1;32m     79\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m grad_norm \u001b[38;5;241m>\u001b[39m epsilon \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m<\u001b[39m max_iters_outer:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# variables.append({'c': c, 'xi': xi, 'w': w, 'b': b}) # uncomment this if you want to see all variables\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     w, b, xi, M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     grad_c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39mupper_grad_x(c)\n\u001b[1;32m     85\u001b[0m     grad_w, grad_b, grad_xi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39mupper_grad_y(c, w, b, xi)\n",
      "File \u001b[0;32m~/Downloads/bilevel_ipm/bilevel_ipm/hyperparam_opt/algorithms/barrier_blo.py:54\u001b[0m, in \u001b[0;36mBarrier_BLO.lower_loop\u001b[0;34m(self, c0, w0, b0, xi0, M, max_iters_inner, epsilon, alpha)\u001b[0m\n\u001b[1;32m     52\u001b[0m converged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m converged:\n\u001b[0;32m---> 54\u001b[0m     w, b, xi, converged\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojected_gradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxi0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iters_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     M \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     56\u001b[0m     w0, b0, xi0 \u001b[38;5;241m=\u001b[39m w, b, xi\n",
      "File \u001b[0;32m~/Downloads/bilevel_ipm/bilevel_ipm/hyperparam_opt/algorithms/barrier_blo.py:33\u001b[0m, in \u001b[0;36mBarrier_BLO.projected_gradient_descent\u001b[0;34m(self, c0, w0, b0, xi0, M, max_iters_inner, epsilon, alpha)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, w: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, b: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, xi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstraints: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem\u001b[38;5;241m.\u001b[39mlower_constraints(c,\u001b[38;5;250m \u001b[39mw,\u001b[38;5;250m \u001b[39mb,\u001b[38;5;250m \u001b[39mxi,\u001b[38;5;250m \u001b[39mm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mquit\u001b[49m()\n\u001b[1;32m     34\u001b[0m i, grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m grad_norm \u001b[38;5;241m>\u001b[39m epsilon \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m<\u001b[39m max_iters_inner:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'quit' is not defined"
     ]
    }
   ],
   "source": [
    "c0, w0, b0, xi0 = np.random.randn(n_train), np.random.randn(45), np.random.randn(), np.random.randn(n_train)\n",
    "from algorithms import barrier_blo\n",
    "for seed in range(10):\n",
    "\n",
    "    x_train, y_train, x_val, y_val, x_test, y_test = train_val_test_split(data, seed, n_train, n_val)\n",
    "    datasets = {\n",
    "        'x_train': x_train, 'y_train': y_train, 'x_val': x_val, \n",
    "        'y_val': y_val, 'x_test': x_test, 'y_test': y_test\n",
    "    }\n",
    "    \n",
    "    problem = barrier_blo.SVM_Problem(datasets)\n",
    "    barrier_blo = barrier_blo.Barrier_BLO(problem, hparams)\n",
    "    \n",
    "    # metrics_seed, variables_seed = barrier_blo(x_train, y_train, x_val, y_val, x_test, y_test, hparams, epochs)\n",
    "    metrics_seed, c, w, b, xi, grad_norm = barrier_blo.upper_loop(c0, w0, b0, xi0, hparams)\n",
    "    metrics.append(metrics_seed)\n",
    "    # variables.append([c, w, b, xi])\n",
    "\n",
    "train_acc = np.array([[x['train_acc'] for x in metrics] for metrics in metrics])\n",
    "val_acc = np.array([[x['val_acc'] for x in metrics] for metrics in metrics])\n",
    "test_acc = np.array([[x['test_acc'] for x in metrics] for metrics in metrics])\n",
    "val_loss = np.array([[x['val_loss'] for x in metrics] for metrics in metrics])\n",
    "test_loss = np.array([[x['test_loss'] for x in metrics] for metrics in metrics])\n",
    "\n",
    "time_computation = np.array([[x['time_computation'] for x in metrics] for metrics in metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
